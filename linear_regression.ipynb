{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn #linear이기 때문에\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init #초기값을 주기위해\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imyeonghui/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/imyeonghui/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "num_data=100\n",
    "num_epoch=100\n",
    "\n",
    "noise=init.normal(torch.FloatTensor(num_data,1),std=1)#mean은 생략\n",
    "x=init.uniform(torch.Tensor(num_data,1),-10,10)#-10에서 10까지 랜덤하게 생성\n",
    "\n",
    "y=2*x+3\n",
    "y_noise=y+noise #학습때 사용할 것은 노이즈가 낀 데이터임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =nn.Linear(1,1)#들어가는 input size, 나오는 output size .지금은 1개 들어오고 1개 나오니깐 (1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(Variable(x))\n",
    "\n",
    "#무엇을 사용할 건지 정의를 먼저 해줌.\n",
    "loss_func=nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2599, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2165, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1875, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1680, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1550, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1463, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1405, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1366, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1340, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1322, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "loss_arr=[]\n",
    "\n",
    "label=Variable(y_noise)#label 값도 output과 같은 Variable이여야 함. 생략하면 gradient가 계속 누적됨.\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad()#update한 gradient값 초기화.\n",
    "    output=model(Variable(x))\n",
    "    \n",
    "    loss=loss_func(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()#update\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(loss)\n",
    "        \n",
    "    loss_arr.append(loss.data.numpy())#loss가 얼마나 줄어들었나 확인하기 위해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9992]]) tensor([3.0065])\n"
     ]
    }
   ],
   "source": [
    "#train된 파라미터를 확인할 수 있음\n",
    "\n",
    "param_list=list(model.parameters())#model.parameters() 모든 파라미터를 다 불러올 수 있음. 이것을 list로 바꾸어서\n",
    "print(param_list[0].data,param_list[1].data)#noise가 꼈는데도 2와 3에 가까운 것을 볼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
